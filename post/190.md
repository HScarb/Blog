title: SVM 支持向量机
date: 2016-12-28 15:18:39
modified: 2016-12-28 15:18:39
author: Scarb
postid: 190
slug: 190
nicename: svm
attachments: 174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189
posttype: post
poststatus: publish
tags: datamining, alg
category: exp

# SVM 支持向量机

* By Scarb
* 数据挖掘课程的期末作业。学期末的时候太忙了，所以没有写博客发上来。隔了这么久终于想起来这个于是发一下。

[TOC]

## 1. 什么是支持向量机(SVM)
### 1.1 一个故事

在介绍SVM是什么之前，先来看一个故事……
我在网络上看到这个故事，通过它很快地理解了SVM的作用。
好吧，故事是这样子的：
在很久以前的情人节，大侠要去救他的爱人，但魔鬼和他玩了一个游戏。
魔鬼在桌子上似乎有规律放了两种颜色的球，说：“你用一根棍分开它们？要求：尽量在放更多球之后，仍然适用。”

![SVM_1][img1]

于是大侠这样放了。

![SVM_2][img2]

SVM就是试图把棍放在最佳位置，好让在棍的两边有尽可能大的间隙。
魔鬼看到大侠已经解决了一个难题，于是魔鬼给了大侠一个新的挑战；这也是SVM中另一个更加重要的难题：

![SVM_3][img3]

现在，大侠没有棍可以很好帮他分开两种球了，现在怎么办呢？当然像所有武侠片中一样大侠桌子一拍，球飞到空中。然后，凭借大侠的轻功，大侠抓起一张纸，插到了两种球的中间。

![SVM_4][img4]

现在，从魔鬼的角度看这些球，这些球看起来像是被一条曲线分开了。

![SVM_5][img5]

再之后，无聊的大人们，把这些球叫做 「数据」，把棍子 叫做 「分类器」, 找到最大间隙的方法叫做「最优化」， 拍桌子叫做「核方法」, 那张纸叫做「超平面」。

### 1.2 SVM 是什么

看了上面这个故事，基本能理解SVM大致是做什么的。它的本质是一个线性分类器。
支持向量机是用来解决分类问题的，它找到一条线将两个特征划分开来，新的数据在线的哪边就可以直接把数据划分进这一类中。所以它是一种对现形和非线性数据进行分类的方法。
简单地说，SVM是一种算法，它使用一种非线性映射，把原训练数据映射到较高的维上（就是大侠用轻功把球打飞起来）。在新的维上，它搜搜最佳分离超平面（即插入一张纸）。使用到足够高维上的、合适的非线性映射，两个类的数据总可以被超平面分开。

## 2. SVM 的原理
### 2.1 数据线性可分的情况
#### 2.1.1 超平面以及点到平面的距离

线性分类器用一个超平面 wx + b = 0 将空间划分为两个部分，wx + b > 0 和 wx + b < 0，w的方向和超平面方向垂直。

![SVM_6][img6]

那么如何求空间上一点到这个超平面的距离呢？经过数学推导可以得到下面的结论

![SVM_7][img7]

这个距离就等于g(x)的绝对值比w的模。所以原点到超平面的距离是b的绝对值比w的模。
这个结论在后面将会用到。

#### 2.1.2 margin、Support Vecotrs 间隔和支持向量

超平面向上或者向下平移，直到遇到某一个点时停下。能够向上和向下平移距离的和就是margin（间隔）。而SVM就是要让这个超平面的margin最大。
Support Vecotrs（支持向量），就是超平面平移时最先遇到的点。这个名字比较形象，支持向量就好像它托着这个面一样。来看一张图：

![SVM_8][img8]

在SVM中有一些新的定义：把分界面定为wx + b = 0，分界面的上边缘定为 wx + b = 1，分界面的下边缘定为 wx + b = -1。超过上边界的数据点记为1，超过下边界的数据点记为-1。
所以可以把SVM的问题转换成求最大margin的问题。那么margin如何得到？通过刚才得出的求某一点到分界面的公式可以得到计算margin的公式，如下图所示：

![SVM_9][img9]

#### 2.1.3 求解最大边缘超平面

接下来就是研究如何把margin最大化。在这之前有一个前提条件就是把数据分类分对。

![SVM_A][imgA]

经过数学计算得出：

![SVM_B][imgB]
![SVM_C][imgC]

### 2.2 数据线性不可分的情况

在某些情况中，两种数据混合呈现线性不可分的状态，不可能找到一条将这些类分开的直线，上面研究的线性SVM不可能找到可行解，这时怎么办？
SVM的解决方法是把数据映射到更高维的空间，这个时候就仍然可以用一条线将这些数据分隔开了。

![SVM_D][imgD]

那么到底应该如何去做映射呢？实际上我们不需要为每个问题设计特别的映射。它实际上使用几种固定的映射方法，映射到一个高维的空间。
但是映射到高维的空间做计算味着计算量会相当大，这不是自讨苦吃吗？
实际上SVM最精妙的地方就在于此。
假设我们将数据映射到高维空间，然后做內积，就变成：

![SVM_E][imgE]

在高维空间做內积的计算量非常大，但是在这有一个惊奇的发现：

![SVM_F][imgF]

在这个公式中可以看到在高维空间中的做的操作就等价于在m维空间上的操作。
这就是SVM中的**核函数K（Kernel）**：这个函数在原始空间上计算出来的值就等于在高维空间上计算出来的值。核函数还有一些其他的类型。

最后仍然要求出w 和b，得到最大边缘超平面：

![SVM_G][imgG]

其实最后的结果和线性可分情况下的公式差不多，区别就在于xi · x 变成了核函数。
SVM的功能十分强大，而它的表达式又十分简洁，这极大地推动了SVM的应用。它能够处理线性不可分的问题，核函数功不可没。

## 3. SVM的优缺点
### 3.1 优点

- 可以解决高维问题，即大型特征空间
- 能够处理非线性特征的相互作用
- 无需依赖整个数据
- 可以提高泛化能力

### 3.2 缺点

- 当观测样本很多时，效率并不是很高
- 对非线性问题没有通用解决方案，有时候很难找到一个合适的核函数
- 对缺失数据敏感

## 4. 参考资料

1. 数据挖掘：理论与算法 在线课程——清华大学 深圳研究生院  袁博
2. 《数据挖掘 概念与技术》——韩家炜、Micheline Kamber、裴健
3. 《Support Vector Machines explained well》——  Iddo

[img1]: http://47.106.131.90/blog/uploads/2017/04/SVM_1.png
[img2]: http://47.106.131.90/blog/uploads/2017/04/SVM_2.png
[img3]: http://47.106.131.90/blog/uploads/2017/04/SVM_3.png
[img4]: http://47.106.131.90/blog/uploads/2017/04/SVM_4.png
[img5]: http://47.106.131.90/blog/uploads/2017/04/SVM_5.png
[img6]: http://47.106.131.90/blog/uploads/2017/04/SVM_6.png
[img7]: http://47.106.131.90/blog/uploads/2017/04/SVM_7.png
[img8]: http://47.106.131.90/blog/uploads/2017/04/SVM_8.png
[img9]: http://47.106.131.90/blog/uploads/2017/04/SVM_9.png
[imgA]: http://47.106.131.90/blog/uploads/2017/04/SVM_A.png
[imgB]: http://47.106.131.90/blog/uploads/2017/04/SVM_B.png
[imgC]: http://47.106.131.90/blog/uploads/2017/04/SVM_C.png
[imgD]: http://47.106.131.90/blog/uploads/2017/04/SVM_D.png
[imgE]: http://47.106.131.90/blog/uploads/2017/04/SVM_E.png
[imgF]: http://47.106.131.90/blog/uploads/2017/04/SVM_F.png
[imgG]: http://47.106.131.90/blog/uploads/2017/04/SVM_G.png